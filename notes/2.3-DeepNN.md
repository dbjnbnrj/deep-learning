2.3-Deep Neural Networks

#Summary
- Building a Multi-Layer NN
- Using DL Network to get Features from Images (Detect Objects in Pictures etc.)
- AWS Credits
- How to use Ec2 to train NN on GPU
- Object Recog Proj. Train CNN using CIFAR to id images from object

# Multilayer NN

ReLU is a non-linear function.

Value = 0 for x < 0
      = x for x > 0



#How to make an Image Classifier

model = Sequential() # Allows us to build a linear stack of layers
        Graph() # Allows multiple seperate input and output

model.add(Convolutional2D())


> Input 32x32x3 (3 is RGB with values between 0-255)
 CNN uses a 3 x 3 x 3 filter
 Filter is a feature identifier that slides around the input it multiplies its values with the image values
 This is called element-wise multiplication.
 Multiplication from each part of the image are then summed up to give an output matrix.
 Weights are randomly initialized. During training CNN will learn value for the filter


model.add(Activation('relu'))

> Input: Input Feature Map
 Output: Rectified Feature Map ( All -ve pixel values converted to zero )
 Notes: It improves the non-linear properties of the model
 Purpose: It enables the model to learn more complex functions

model.add(MaxPooling2D(pool_size=(2,2)))

> Input: Rectified Feature Map
 Output: 2 x 2 x 3 Feature Map
 Notes: Reduces dimensionality of the feature map
 Purpose: Reduces computational complexity of the network


# Classic CNN architecture uses these 3 layers repeatedly

# Dropout layer sets a random number of activations to 0 to prevent overfitting


To Prevent Overfitting - Add dropout layer
Dropout layer sets random activations to 0 to
Flatten the feature map to 1D

model.add(Flatten())  # 1 D
model.add(Dense(64)) # Initializes a fully connected layer
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

How to improve accuracy
- More Data
- Transfer Learning


Why multilayer NN?
- Parameter Efficiency
- Hierarchial Structure similar to natural phenomenom (points to lines edges to objects )


# Save and Restore tf Models


# Saving variables
`import tensorflow as tf

save_file = './model.ckpt'

weights = tf.Variable(tf.truncated_normal([2,3]))
bias = tf.Variable(tf.truncated_normal([3]))

saver = tf.train.Saver()

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  sess.run(weights)
  sess.run(bias)

  saver.save(sess, save_file)

tf.reset_default_graph()

weights = tf.Variable(tf.truncated_normal([2,3]))
bias = tf.Variable(tf.truncated_normal([3]))

saver = tf.train.Saver()

with tf.Session() as sess:
  saver.restore(sess, save_file)
  sess.run(weights)
  sess.run(bias)`

# Finetuning the model

Order of initialization is important when you are loading the file because of TF internal
Variable name conversion

To prevent this from happening, initialize the name along with the Variable

weights = tf.Variable( tf.truncated_normal([2,3]) , name='weights_0')
weights = tf.Variable( tf.truncated_normal([3]) , name='bias_0')


# Regularization

Overview
- Deep models work only if you have large datasets
- Network which is the right size for your data is very difficult to optimize
- We train networks which are way too bug for data and then we prevent overfitting

## How to prevent overfitting

- Early Termination : Look at Performance VS Validation Set. Stop training as soon as we stop improving training Set

- Regularization: Applying artificial constraints on your network to make it more flexible

L2 Regularization (new hyperparameter): Add term to loss to penalize large weights

L2  = 1/2 (w1^2 + w2^2 + ... wn^2)

L2 derivative = (w1 + w2 +.... wn)


## Dropout

Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units' incoming and outgoing connections.

If dropout doesn't work use a bigger network

Smaller network w/o dropout < Bigger network with dropout

To average result of evaluation to be similar to result for training, double the weights of the training dataset for the weights that have not been randomly initialized to 0.

ye (Output of evaluation dataset) ~ E(yt) (Average of training dataset)

Overview -
The tf.nn.dropout() function takes in two parameters:

hidden_layer: the tensor to which you would like to apply dropout
keep_prob: the probability of keeping (i.e. not dropping) any given unit

keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.


keep_prob = 0.5 (for training)
         = 1.0 (for test)


Dropout Code -
~~~~
# Solution is available in the other "solution.py" tab
import tensorflow as tf

hidden_layer_weights = [
    [0.1, 0.2, 0.4],
    [0.4, 0.6, 0.6],
    [0.5, 0.9, 0.1],
    [0.8, 0.2, 0.8]]
out_weights = [
    [0.1, 0.6],
    [0.2, 0.1],
    [0.7, 0.9]]

# Weights and biases
weights = [
    tf.Variable(hidden_layer_weights),
    tf.Variable(out_weights)]
biases = [
    tf.Variable(tf.zeros(3)),
    tf.Variable(tf.zeros(2))]

# Input
features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])


keep_prob = tf.placeholder(tf.float32)
hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])
hidden_layer = tf.nn.relu(hidden_layer)
hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)
logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(logits, feed_dict={keep_prob: 0.5}))
~~~~
