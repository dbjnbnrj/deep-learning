#2.4 - Convolutional Neural Networks


##Introduction to Comvolutional Networks

Purpose: Primarily used to train image data


### Features

A) Color -

- May not be important while dealing with images if we only care about structure (eg: images of alphabets/numbers)

B) Statistical Invariance
  - **Transitional invariance** : Position of the object in the image is not important.
  - **Weight Sharing** - when 2 inputs contain the same value apply the same weights (eg: position of object in image, same word in different sentences)


### CNN

Q) What is it?

A) NN that shares its parameters across space

Q) What is the input ?

A) Similar to a flat pancake. An image with width, height and depth (RGB values)

Q) What is a convolution ?

A)
- We divide the input into small patches
- We slide a NN over each patch, the NN has K outputs.
- We slide this NN across and vertically
- The output has a different (smaller) width and height and (larger ) depth.
- This process is a Convolutions

The overall network looks like a pyramid
![pyramid](./cnn.png, "Convolutional Neural Networks")


#### Definitions:
- Patch/Kernel: Region of the image which you are concentrating on

- Pancake: Images with no depth (Red filter image). Each pancake is a feature map

- Stride: No of pixels that your shifting when you move your filter
Eg: Stride of 1 is roughly same as input/Stride of 2 is half of the input

- Valid Padding: When you don't go off the edge

- Same padding: When you go off the edge and pad with zeros so that the output map size is the same as the input


#### Intuition:

- A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects.

- The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors.

- The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.

#### Step 1: Filters

- Break the image up into smaller patches (select a width and a height that defines a filter)
- Patch size = Filter size
- Slide the filter across the image (vertically OR horizontally). Note that are grouping together adjacent pixels and treating them as a collective.
-  By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image.
- Stride = Distance by which filter slides
- Greater Stride = Smaller models = Reduction in Accuracy
- Smaller Stride = Larger/More accurate models
- There may be more than one filter to pick up different qualities of a patch
- Filter Depth: The number of filters in a convolutional layer is called the filter depth.

Q) How many neurons does each patch connect to?

A)

That’s dependent on our filter depth. If we have a depth of k, we connect each patch of pixels to k neurons in the next layer. This gives us the height of k in the next layer, as shown below. In practice, k is a hyperparameter we tune, and most CNNs tend to pick the same starting values.

Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture.

#### Step 2: Feature Maps

Feature Map Size Calculation:
- Same Padding : Width = Width /Stride ; Height = Height/Stride ; Depth = Output Depth
- Valid Padding : Width = (Width - 2)/Stride ; Height = (Height- 2)/Stride ; Depth = Output Depth



the following formula gives us the width of the next layer: W_out = (W−F+2P)/S+1.

The output height would be H_out = (H-F+2P)/S + 1.

And the output depth would be equal to the number of filters D_out = K.

The output volume would be W_out * H_out * D_out.


 ** Tensor Flow Formula **

```input = tf.placeholder(tf.float32, (None, 32, 32, 3))
filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)
filter_bias = tf.Variable(tf.zeros(20))
strides = [1, 2, 2, 1] # (batch, height, width, depth)
padding = 'SAME'
conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias
```

#### Step 3: Parameters

**Transitional Invariance**
The weights, w, are shared across patches for a given layer in a CNN to detect the object above regardless of where in the image it is located.

Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.

 If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images.



#### Visualizing CNN

~~~~Code


weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, color_channels, k_output]))
bias = tf.Variable(tf.zeros(k_output))

conv_layer = tf.nn.conv2d(input, weight, stride=[1,2,2,1], padding='SAME')

# stride= (batch, height, width, depth)
conv_layer = tf.nn.bias_add(conv_layer, bias)
conv_layer = tf.nn.relu(conv_layer)
~~~~


#### Improvements

Advanced Convnet Concepts

A. Pooling

We want to reduce the spatial extent of future maps in the convolutional pyramid.
Striding shifts the filters by a few pixels each time to reduce feature map size. This is a very aggressive way of sampling down an image.

Pooling involves taking all the convolutions in a neighbourhood and combining them.
Max-pooling involves taking the maximum of all the values in the feature map.

Advantages
- Parameter free
- Often more accurate
- More expensive: As convolution layer below has lower stride
- More hyperparameters (Pooling size and pooling stride)

General Convnet

Image - Conv+MaxPool x 2 - Fully Connected x 2 - Classifier

TensorFlow Code
```
conv_layer = tf.nn.conv2d(input, weight, strides=[1,2,2,1], padding="SAME")
conv_layer = tf.nn.bias_add(conv_layer, bias)
conv_layer = tf.nn.relu(conv_layer)

conv_layer = tf.nn.max_pool(conv_layer, ksize=[1,2,2,1], strides=[1,2,2,1], padding="SAME")
```
eg: Lenet-5; AlexNet

Advantages

- Decreases the size of the output and prevent overfitting.
- Preventing overfitting is a consequence of reducing the output size, which in turn, reduces the number of parameters in future layers.


B. 1x1 Convolutions

Normal CNN is a linear classifier for an image.
1x1 Convolution interspersed in the networks helps by introducing non-linearity in an inexpensive way without changing the structure of the network.


C. Inception

Instead of having a single CNN you can have a combination of 1x1, 3x3 and MaxPooling


Quiz: TensorFlow CNN

```"""
Setup the strides, padding and filter weight/bias such that
the output shape is (1, 2, 2, 3).
"""
import tensorflow as tf
import numpy as np

# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)
# (1, 4, 4, 1)
x = np.array([
    [0, 1, 0.5, 10],
    [2, 2.5, 1, -8],
    [4, 0, 5, 6],
    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))
X = tf.constant(x)


def conv2d(input):
    # Filter (weights and bias)
    # The shape of the filter weight is (height, width, input_depth, output_depth)
    # The shape of the filter bias is (output_depth,)
    # TODO: Define the filter weights `F_W` and filter bias `F_b`.
    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.
    F_W = tf.Variable(tf.random_normal([4, 4, 1, 3 ]))
    F_b = tf.Variable(tf.random_normal([3]))
    # TODO: Set the stride for each dimension (batch_size, height, width, depth)
    strides = [1, 2, 2, 1]
    # TODO: set the padding, either 'VALID' or 'SAME'.
    padding = "SAME"
    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d
    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.
    return tf.nn.conv2d(input, F_W, strides, padding) + F_b

out = conv2d(X)
```


QUIZ: Max Pooling

```"""
Set the values to `strides` and `ksize` such that
the output shape after pooling is (1, 2, 2, 1).
"""
import tensorflow as tf
import numpy as np

# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)
# (1, 4, 4, 1)
x = np.array([
    [0, 1, 0.5, 10],
    [2, 2.5, 1, -8],
    [4, 0, 5, 6],
    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))
X = tf.constant(x)

def maxpool(input):
    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)
    ksize = [1, 2, 2, 1]
    # TODO: Set the stride for each dimension (batch_size, height, width, depth)
    strides = [1, 2, 2, 1]
    # TODO: set the padding, either 'VALID' or 'SAME'.
    padding = "VALID"
    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool
    return tf.nn.max_pool(input, ksize, strides, padding)

out = maxpool(X)````
