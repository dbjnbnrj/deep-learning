#Lesson 2.1

# Intro to TF

## Install tensorflow

conda create -n tensorflow python=3.5
source activate tensorflow
conda install pandas matplotlib jupyter notebook scipy scikit-learn
pip install tensorflow

## Check if TF is installed

~~~~
import tensorflow as tf

# creates 0-D
hello_constant = tf.constant("hello world")

# evaluate tensor in a session
# session is environment in which tensor runs
with tf.Session() as sess:
  output = sess.run(hello_constant)
~~~~

### Handle dynamic datasets

~~~~
x = tf.placeholder(tf.string)
with tf.Session() as sess:
  output = sess.run(x, feed_dict={x:"Hello World"})

y = tf.placeholder(tf.int32)
z = tf.placeholder(tf.float32)
with tf.Session() as sess:
  output = sess.run(x, feed_dict={x:"Test String", y: 123, z:34.78})
~~~~

### RECAP

- Ran operations in tf.Session.
- Created a constant tensor with tf.constant().
- Used tf.placeholder() and feed_dict to get input.
- Applied the tf.add(), tf.subtract(), tf.multiply(), and tf.divide() functions using numeric data.
- Learned about casting between types with tf.cast()

### Classification
- Supervised: Given data + existing labels detect class of new data
- Logistic Classifier: Takes the input and applies a linear function (matrix multiply
                       to them to generate its predictions.
      Wx + b = y [Train the Weights + Bias to perform predictions properly]
      To turn scores to probabilities we use softmax function.
      It can turn any score and convert it to proper probabilities.
      e{pow(yi)}\sum(e{pow(yj)})
- Use tf.Variable() to create tensors for weight and bias and update them

~~~~
# Initialize the values in the tf graph
init = tf.global_variables_initializer()

n_features = 120
n_labels = 5

# tf.truncated_normal() generates random numbers from normal distribution
weights = tf.Variable(tf.truncated_normal(n_features, n_labels))

n_labels = 5
# tf.zeros returns a tensor with all zeroes
bias = tf.Variable(tf.zeros(n_labels))

with tf.Session() as sess:
  sess.run(init)
~~~~

## TF Softmax
The softmax function squashes it's inputs, typically called logits or logit scores,
to be between 0 and 1 and also normalizes the outputs such that they all sum to 1.
This means the output of the softmax function is equivalent to a categorical
probability distribution.

## One hot encoding
Output is represented as a vector where a value of 1.0 is assigned to the correct class
and a value of 0.0 is assigned to all other classes.

## Cross Entropy Loss
Making sure your prediction is correct
https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589b18f5_cross-entropy-diagram/cross-entropy-diagram.png

y'                                          y
[0.1      D[y', y] = -SUM(yj * log(y'i))   [0.0
0.5                                         1.0
0.4]                                        0.0]

## Minimizing Cross Entropy
Find weights and biases to get low distance from the correct class and high distance from incorrect classes

Total Loss = 1/N * SUM( Distance(Softmax(W * x + b)), Lossi)

Minimize the total loss for the training set
Training loss is the average cross entropy function


## Numerical Stability
We do not want values that calculate the loss function to be too big or too small as this will skew the calculations
How do we do that?
- We make sure variables have 0 mean and equal variance wherever possible
- If problem is badly conditioned the optimizer has to do a lot of searching to find a solution
- If problem is well conditioned it is easier for the optimizer


## Algorithm
For images -
- Normalize : RGB - 128 / 128
- Initialize weights and biases
- Weights should be drawn from a std deviation
  If there is large variance (sigma) there will have large peak
  Small sigma means you will be uncertain about things
- Apply Softmax
- Apply CE Loss
- Optimization calculates change in weight + bias
- Iterate till minimum of Loss Function is reached

## Measuring Performance
- Use training, test and validation sets helps you measure how well your doing


## Optimizing a Logistic Classifier
- Gradient Descent optimizes error measure
- However this is difficult to scale

## Stochastic GD
- Calculating GD is expensive as it requires sum/svg over all elements in your set
- Instead compute estimate of the loss (random loss for a random fraction of the data)
- Compute the derivative for the random sample
- SGD scales well with both data + model size

## Momentum and Learning Rate
- Make input 0 mean and equal variance
- Init random weights with small varianc
- In each case we take step in small direction
- Keep running avg of the gradients
- Momentum technique leads to better convergence
- Take smaller noisier steps towards convergence
- Make step smaller as you train (Tune the LR)

## Parameter Hyperspace
- Learning Rate counterintuitive (higher LR starts wells and plateau)
- When things don't work reduce LR
- ADAGRAD: modification of SGD that does momentum and LR decay for us


## Mini-batching
 - Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time.
 - This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.

## Epoch
- An epoch is a single forward and backward pass of the whole dataset.
- This is used to increase the accuracy of the model without requiring more data.
- This section will cover epochs in TensorFlow and how to choose the right number of epochs.
